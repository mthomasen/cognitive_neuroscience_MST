<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <bib:Book rdf:about="#item_497">
        <z:itemType>book</z:itemType>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>CreateSpace</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Rossum</foaf:surname>
                        <foaf:givenName>Guido</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Drake</foaf:surname>
                        <foaf:givenName>Fred L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Python 3 Reference Manual</dc:title>
        <dc:date>2009</dc:date>
    </bib:Book>
    <bib:Article rdf:about="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2869.1996.00220.x">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1365-2869"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sallinen</foaf:surname>
                        <foaf:givenName>Mikael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaartinen</foaf:surname>
                        <foaf:givenName>Jukka</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lyytinen</foaf:surname>
                        <foaf:givenName>Heikki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_555"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>auditory stimuli</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>event-related potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>phasic</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>REM sleep</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>tonic</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Processing of auditory stimuli during tonic and phasic periods of REM sleep as revealed by event-related brain potentials</dc:title>
        <dcterms:abstract>The brain has been reported to be more preoccupied with dreams during phasic than during tonic REM sleep. Whether these periods also differ in terms of the processing of external stimuli was examined. Event-related brain potentials (ERPs) to a frequent standard tone of 1000 Hz (P= 97%) and infrequent deviant tones of 1100 and 2000 Hz (P= 1.5% for each) were recorded (n= 13) during wakefulness and nocturnal sleep. An ERP wave (called REM-P3) resembling a waking P3 wave was larger for the 2000 Hz deviant during tonic than during phasic REM sleep. Also the P210 wave was larger during tonic than during phasic REM sleep. A reliable mismatch negativity component appeared only in wakefulness. In summary, these results support the hypothesis that the brain is more ‘open’ for changes in an auditory input during tonic than phasic REM sleep.</dcterms:abstract>
        <dc:date>1996</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Wiley Online Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2869.1996.00220.x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-04-26 11:29:36</dcterms:dateSubmitted>
        <dc:description>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2869.1996.00220.x</dc:description>
        <bib:pages>220-228</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1365-2869">
        <prism:volume>5</prism:volume>
        <dc:title>Journal of Sleep Research</dc:title>
        <dc:identifier>DOI 10.1111/j.1365-2869.1996.00220.x</dc:identifier>
        <prism:number>4</prism:number>
        <dc:identifier>ISSN 1365-2869</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_555">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/555/Sallinen et al. - 1996 - Processing of auditory stimuli during tonic and ph.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1365-2869.1996.00220.x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-04-26 11:29:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://linkinghub.elsevier.com/retrieve/pii/0013469494900353">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:00134694"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sallinen</foaf:surname>
                        <foaf:givenName>Mikael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaartinen</foaf:surname>
                        <foaf:givenName>Jukka</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lyytinen</foaf:surname>
                        <foaf:givenName>Heikki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_556"/>
        <dc:title>Is the appearance of mismatch negativity during stage 2 sleep related to the elicitation of K-complex?</dc:title>
        <dc:date>8/1994</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://linkinghub.elsevier.com/retrieve/pii/0013469494900353</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-04-26 11:30:04</dcterms:dateSubmitted>
        <bib:pages>140-148</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:00134694">
        <prism:volume>91</prism:volume>
        <dc:title>Electroencephalography and Clinical Neurophysiology</dc:title>
        <dc:identifier>DOI 10.1016/0013-4694(94)90035-3</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Electroencephalography and Clinical Neurophysiology</dcterms:alternative>
        <dc:identifier>ISSN 00134694</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_556">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/556/Sallinen et al. - 1994 - Is the appearance of mismatch negativity during st.pdf"/>
        <dc:title>Sallinen et al. - 1994 - Is the appearance of mismatch negativity during st.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://d1wqtxts1xzle7.cloudfront.net/83332109/0013-4694_2894_2990035-320220406-10385-16qr78m-libre.pdf?1649264237=&amp;response-content-disposition=inline%3B+filename%3DIs_the_appearance_of_mismatch_negativity.pdf&amp;Expires=1682512189&amp;Signature=Fg5FLmkG~EKhMw5Tp45FZUIwZ5-q2HSkFPWrrMOGbFml8uRgBE~-wiiO2zec7zH~xILmWprGEVcAFonoe43JmOE8rCEaPT05~OO~VUtBKb1djraQAIxyeChMWbv3FSWYF-k~3PKqnpPBTbvwZQSzs73LBEBQSfbisUVxdotYR-yipeRifVPvR0kl98YTHDaTcd29GSr-h58ZDdxtE7-Qj5uaPQPE0b60CZzb7nOYdbhEqHl4cclbWhfOI4P0PNEcS4q2Mts6mBuPLjmhsfIv2K~YCjgKmG2RQfzbCPBnZ1HMythTk05wtV82FmSsiHRHgZ91V99awfPnIBjyQ8-sqg__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-04-26 11:30:01</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0301051106001402">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0301-0511"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kujala</foaf:surname>
                        <foaf:givenName>Teija</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tervaniemi</foaf:surname>
                        <foaf:givenName>Mari</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schröger</foaf:surname>
                        <foaf:givenName>Erich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_563"/>
        <link:link rdf:resource="#item_564"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Auditory processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Event-related potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Mismatch negativity</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Sensory memory</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Theme: sensory systems</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Topic: auditory systems: central physiology</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>The mismatch negativity in cognitive and clinical neuroscience: Theoretical and methodological considerations</dc:title>
        <dcterms:abstract>Mismatch negativity (MMN) component of the event-related brain potentials has become popular in cognitive and clinical brain research during the recent years. It is an early response to a violation of an auditory rule such as an infrequent change in the physical feature of a repetitive sound. There is a lot of evidence on the association of the MMN parameters and behavioral discrimination ability, although this relationship is not always straight-forward. Since the MMN reflects sound discrimination accuracy, it can be used for probing how well different groups of individuals perceive sound differences, and how training or remediation affects this ability. In the present review, we first introduce some of the essential MMN findings in probing sound discrimination, memory, and their deficits. Thereafter, issues which need to be taken into account in MMN investigations as well as new improved recording paradigms are discussed.</dcterms:abstract>
        <dc:date>2007-01-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>The mismatch negativity in cognitive and clinical neuroscience</z:shortTitle>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0301051106001402</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-04-26 11:37:46</dcterms:dateSubmitted>
        <bib:pages>1-19</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0301-0511">
        <prism:volume>74</prism:volume>
        <dc:title>Biological Psychology</dc:title>
        <dc:identifier>DOI 10.1016/j.biopsycho.2006.06.001</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Biological Psychology</dcterms:alternative>
        <dc:identifier>ISSN 0301-0511</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_563">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/563/Kujala et al. - 2007 - The mismatch negativity in cognitive and clinical .pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0301051106001402/pdfft?md5=e79558d6a226253b709c87c0a4e938f7&amp;pid=1-s2.0-S0301051106001402-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-04-26 11:37:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_564">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/564/S0301051106001402.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0301051106001402</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-04-26 11:37:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.frontiersin.org/articles/10.3389/fnhum.2014.00666">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1662-5161"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stefanics</foaf:surname>
                        <foaf:givenName>Gábor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kremláček</foaf:surname>
                        <foaf:givenName>Jan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Czigler</foaf:surname>
                        <foaf:givenName>István</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_572"/>
        <dc:title>Visual mismatch negativity: a predictive coding view</dc:title>
        <dcterms:abstract>An increasing number of studies investigate the visual mismatch negativity (vMMN) or use the vMMN as a tool to probe various aspects of human cognition. This paper reviews the theoretical underpinnings of vMMN in the light of methodological considerations and provides recommendations for measuring and interpreting the vMMN. The following key issues are discussed from the experimentalist's point of view in a predictive coding framework: (1) experimental protocols and procedures to control “refractoriness” effects; (2) methods to control attention; (3) vMMN and veridical perception.</dcterms:abstract>
        <dc:date>2014</dc:date>
        <z:shortTitle>Visual mismatch negativity</z:shortTitle>
        <z:libraryCatalog>Frontiers</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fnhum.2014.00666</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-05-20 13:30:26</dcterms:dateSubmitted>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1662-5161">
        <prism:volume>8</prism:volume>
        <dc:title>Frontiers in Human Neuroscience</dc:title>
        <dc:identifier>ISSN 1662-5161</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_572">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/572/Stefanics et al. - 2014 - Visual mismatch negativity a predictive coding vi.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fnhum.2014.00666/pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-05-20 13:30:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Data rdf:about="#item_589">
        <z:itemType>computerProgram</z:itemType>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Adobe Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <z:programmers>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Christiansen</foaf:surname>
                        <foaf:givenName>M</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </z:programmers>
        <dc:title>Adobe After Effects CC Visual Effects and Compositing Studio Techniques</dc:title>
        <dc:date>2013</dc:date>
    </bib:Data>
    <bib:Data rdf:about="#item_590">
        <z:itemType>computerProgram</z:itemType>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Image-Line Software NV</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <z:programmers>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fisher</foaf:surname>
                        <foaf:givenName>Scott</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vanmol</foaf:surname>
                        <foaf:givenName>Frederic</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krajcovic</foaf:surname>
                        <foaf:givenName>Miroslav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dambrin</foaf:surname>
                        <foaf:givenName>Didier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </z:programmers>
        <prism:edition>20</prism:edition>
        <dc:title>Fl Studio 20</dc:title>
        <dc:date>2022</dc:date>
    </bib:Data>
    <bib:Article rdf:about="https://doi.org/10.3758/s13428-018-01193-y">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1554-3528"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Peirce</foaf:surname>
                        <foaf:givenName>Jonathan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gray</foaf:surname>
                        <foaf:givenName>Jeremy R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Simpson</foaf:surname>
                        <foaf:givenName>Sol</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>MacAskill</foaf:surname>
                        <foaf:givenName>Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Höchenberger</foaf:surname>
                        <foaf:givenName>Richard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sogo</foaf:surname>
                        <foaf:givenName>Hiroyuki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kastman</foaf:surname>
                        <foaf:givenName>Erik</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lindeløv</foaf:surname>
                        <foaf:givenName>Jonas Kristoffer</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_592"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Experiment</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Open science</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Open-source</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Psychology</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Reaction time</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Software</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Timing</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>PsychoPy2: Experiments in behavior made easy</dc:title>
        <dcterms:abstract>PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.</dcterms:abstract>
        <dc:date>2019-02-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>PsychoPy2</z:shortTitle>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.3758/s13428-018-01193-y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-01 11:26:02</dcterms:dateSubmitted>
        <bib:pages>195-203</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1554-3528">
        <prism:volume>51</prism:volume>
        <dc:title>Behavior Research Methods</dc:title>
        <dc:identifier>DOI 10.3758/s13428-018-01193-y</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Behav Res</dcterms:alternative>
        <dc:identifier>ISSN 1554-3528</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_592">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/592/Peirce et al. - 2019 - PsychoPy2 Experiments in behavior made easy.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.3758%2Fs13428-018-01193-y.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-01 11:26:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.3758/BF03196680">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1943-393X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Woodman</foaf:surname>
                        <foaf:givenName>Geoffrey F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_594"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Clinical Neurophysiology</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Contingent Negative Variation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Lateralized Readiness Potential</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Transcranial Magnetic Stimulation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Visual Search</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>A brief introduction to the use of event-related potentials in studies of perception and attention</dc:title>
        <dcterms:abstract>Because of the precise temporal resolution of electrophysiological recordings, the event-related potential (ERP) technique has proven particularly valuable for testing theories of perception and attention. Here, I provide a brief tutorial on the ERP technique for consumers of such research and those considering the use of human electrophysiology in their own work. My discussion begins with the basics regarding what brain activity ERPs measure and why they are well suited to reveal critical aspects of perceptual processing, attentional selection, and cognition, which are unobservable with behavioral methods alone. I then review a number of important methodological issues and often-forgotten facts that should be considered when evaluating or planning ERP experiments.</dcterms:abstract>
        <dc:date>2010-11-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.3758/BF03196680</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-03 10:35:43</dcterms:dateSubmitted>
        <bib:pages>2031-2046</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1943-393X">
        <prism:volume>72</prism:volume>
        <dc:title>Attention, Perception, &amp; Psychophysics</dc:title>
        <dc:identifier>DOI 10.3758/BF03196680</dc:identifier>
        <prism:number>8</prism:number>
        <dcterms:alternative>Attention, Perception, &amp; Psychophysics</dcterms:alternative>
        <dc:identifier>ISSN 1943-393X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_594">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/594/Woodman - 2010 - A brief introduction to the use of event-related p.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.3758%2FBF03196680.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-03 10:35:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.frontiersin.org/articles/10.3389/fnins.2013.00267">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1662-453X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gramfort</foaf:surname>
                        <foaf:givenName>Alexandre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Luessi</foaf:surname>
                        <foaf:givenName>Martin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Larson</foaf:surname>
                        <foaf:givenName>Eric</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Engemann</foaf:surname>
                        <foaf:givenName>Denis</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Strohmeier</foaf:surname>
                        <foaf:givenName>Daniel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brodbeck</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goj</foaf:surname>
                        <foaf:givenName>Roman</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jas</foaf:surname>
                        <foaf:givenName>Mainak</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brooks</foaf:surname>
                        <foaf:givenName>Teon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Parkkonen</foaf:surname>
                        <foaf:givenName>Lauri</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hämäläinen</foaf:surname>
                        <foaf:givenName>Matti</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_598"/>
        <dc:title>MEG and EEG data analysis with MNE-Python</dc:title>
        <dcterms:abstract>Magnetoencephalography and electroencephalography (M/EEG) measure the weak electromagnetic signals generated by neuronal activity in the brain. Using these signals to characterize and locate neural activation in the brain is a challenge that requires expertise in physics, signal processing, statistics, and numerical methods. As part of the MNE software suite, MNE-Python is an open-source software package that addresses this challenge by providing state-of-the-art algorithms implemented in Python that cover multiple methods of data preprocessing, source localization, statistical analysis, and estimation of functional connectivity between distributed brain regions. All algorithms and utility functions are implemented in a consistent manner with well-documented interfaces, enabling users to create M/EEG data analysis pipelines by writing Python scripts. Moreover, MNE-Python is tightly integrated with the core Python libraries for scientific comptutation (NumPy, SciPy) and visualization (matplotlib and Mayavi), as well as the greater neuroimaging ecosystem in Python via the Nibabel package. The code is provided under the new BSD license allowing code reuse, even in commercial products. Although MNE-Python has only been under heavy development for a couple of years, it has rapidly evolved with expanded analysis capabilities and pedagogical tutorials because multiple labs have collaborated during code development to help share best practices. MNE-Python also gives easy access to preprocessed datasets, helping users to get started quickly and facilitating reproducibility of methods by other researchers. Full documentation, including dozens of examples, is available at http://martinos.org/mne.</dcterms:abstract>
        <dc:date>2013</dc:date>
        <z:libraryCatalog>Frontiers</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fnins.2013.00267</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-03 11:06:14</dcterms:dateSubmitted>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1662-453X">
        <prism:volume>7</prism:volume>
        <dc:title>Frontiers in Neuroscience</dc:title>
        <dc:identifier>ISSN 1662-453X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_598">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/598/Gramfort et al. - 2013 - MEG and EEG data analysis with MNE-Python.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fnins.2013.00267/pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-03 11:06:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1309.0238">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Buitinck</foaf:surname>
                        <foaf:givenName>Lars</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Louppe</foaf:surname>
                        <foaf:givenName>Gilles</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Blondel</foaf:surname>
                        <foaf:givenName>Mathieu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pedregosa</foaf:surname>
                        <foaf:givenName>Fabian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mueller</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grisel</foaf:surname>
                        <foaf:givenName>Olivier</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Niculae</foaf:surname>
                        <foaf:givenName>Vlad</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Prettenhofer</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gramfort</foaf:surname>
                        <foaf:givenName>Alexandre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grobler</foaf:surname>
                        <foaf:givenName>Jaques</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Layton</foaf:surname>
                        <foaf:givenName>Robert</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vanderplas</foaf:surname>
                        <foaf:givenName>Jake</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Joly</foaf:surname>
                        <foaf:givenName>Arnaud</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Holt</foaf:surname>
                        <foaf:givenName>Brian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Varoquaux</foaf:surname>
                        <foaf:givenName>Gaël</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_602"/>
        <link:link rdf:resource="#item_603"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Mathematical Software</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>API design for machine learning software: experiences from the scikit-learn project</dc:title>
        <dcterms:abstract>Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.</dcterms:abstract>
        <dc:date>2013-09-01</dc:date>
        <z:shortTitle>API design for machine learning software</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1309.0238</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-06 08:46:25</dcterms:dateSubmitted>
        <dc:description>arXiv:1309.0238 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.1309.0238</dc:identifier>
        <prism:number>arXiv:1309.0238</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_602">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/602/Buitinck et al. - 2013 - API design for machine learning software experien.pdf"/>
        <dc:title>arXiv Fulltext PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1309.0238.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-06 08:46:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_603">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/603/1309.html"/>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1309.0238</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-06 08:46:33</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="#item_604">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:title>2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</dc:title>
                <dc:identifier>DOI 10.1109/EMBC.2015.7319296</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winkler</foaf:surname>
                        <foaf:givenName>Irene</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Debener</foaf:surname>
                        <foaf:givenName>Stefan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Müller</foaf:surname>
                        <foaf:givenName>Klaus-Robert</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tangermann</foaf:surname>
                        <foaf:givenName>Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_606"/>
        <link:link rdf:resource="#item_605"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Accuracy</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Electroencephalography</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Electrooculography</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Filtering</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Independent component analysis</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Signal to noise ratio</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Standards</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>On the influence of high-pass filtering on ICA-based artifact reduction in EEG-ERP</dc:title>
        <dcterms:abstract>Standard artifact removal methods for electroencephalographic (EEG) signals are either based on Independent Component Analysis (ICA) or they regress out ocular activity measured at electrooculogram (EOG) channels. Successful ICA-based artifact reduction relies on suitable pre-processing. Here we systematically evaluate the effects of high-pass filtering at different frequencies. Offline analyses were based on event-related potential data from 21 participants performing a standard auditory oddball task and an automatic artifactual component classifier method (MARA). As a pre-processing step for ICA, high-pass filtering between 1-2 Hz consistently produced good results in terms of signal-to-noise ratio (SNR), single-trial classification accuracy and the percentage of `near-dipolar' ICA components. Relative to no artifact reduction, ICA-based artifact removal significantly improved SNR and classification accuracy. This was not the case for a regression-based approach to remove EOG artifacts.</dcterms:abstract>
        <dc:date>2015-08</dc:date>
        <z:libraryCatalog>IEEE Xplore</z:libraryCatalog>
        <dc:description>ISSN: 1558-4615</dc:description>
        <bib:pages>4101-4105</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_606">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/606/stamp.html"/>
        <dc:title>IEEE Xplore Abstract Record</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319296</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-06 09:03:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_605">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/605/Winkler et al. - 2015 - On the influence of high-pass filtering on ICA-bas.pdf"/>
        <dc:title>IEEE Xplore Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&amp;arnumber=7319296&amp;ref=</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-06 09:03:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_608">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zakeri</foaf:surname>
                        <foaf:givenName>Zohreh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_607"/>
        <dc:title>Optimised use of independent component analysis for EEG signal processing</dc:title>
        <dc:date>2016</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_607">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/607/Zakeri - Optimised use of independent component analysis fo.pdf"/>
        <dc:title>Zakeri - Optimised use of independent component analysis fo.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://core.ac.uk/download/pdf/83926209.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-06 09:10:30</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.pnas.org/doi/full/10.1073/pnas.0303760101">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>101</prism:volume>
                <dc:title>Proceedings of the National Academy of Sciences</dc:title>
                <dc:identifier>DOI 10.1073/pnas.0303760101</dc:identifier>
                <prism:number>17</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jääskeläinen</foaf:surname>
                        <foaf:givenName>Iiro P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ahveninen</foaf:surname>
                        <foaf:givenName>Jyrki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bonmassar</foaf:surname>
                        <foaf:givenName>Giorgio</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dale</foaf:surname>
                        <foaf:givenName>Anders M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ilmoniemi</foaf:surname>
                        <foaf:givenName>Risto J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Levänen</foaf:surname>
                        <foaf:givenName>Sari</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Fa-Hsuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>May</foaf:surname>
                        <foaf:givenName>Patrick</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Melcher</foaf:surname>
                        <foaf:givenName>Jennifer</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stufflebeam</foaf:surname>
                        <foaf:givenName>Steven</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tiitinen</foaf:surname>
                        <foaf:givenName>Hannu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belliveau</foaf:surname>
                        <foaf:givenName>John W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_610"/>
        <dc:title>Human posterior auditory cortex gates novel sounds to consciousness</dc:title>
        <dcterms:abstract>Life or death in hostile environments depends crucially on one's ability to detect and gate novel sounds to awareness, such as that of a twig cracking under the paw of a stalking predator in a noisy jungle. Two distinct auditory cortex processes have been thought to underlie this phenomenon: (i) attenuation of the so-called N1 response with repeated stimulation and (ii) elicitation of a mismatch negativity response (MMN) by changes in repetitive aspects of auditory stimulation. This division has been based on previous studies suggesting that, unlike for the N1, repetitive “standard” stimuli preceding a physically different “novel” stimulus constitute a prerequisite to MMN elicitation, and that the source loci of MMN and N1 are different. Contradicting these findings, our combined electromagnetic, hemodynamic, and psychophysical data indicate that the MMN is generated as a result of differential adaptation of anterior and posterior auditory cortex N1 sources by preceding auditory stimulation. Early (≈85 ms) neural activity within posterior auditory cortex is adapted as sound novelty decreases. This alters the center of gravity of electromagnetic N1 source activity, creating an illusory difference between N1 and MMN source loci when estimated by using equivalent current dipole fits. Further, our electroencephalography data show a robust MMN after a single standard event when the interval between two consecutive novel sounds is kept invariant. Our converging findings suggest that transient adaptation of feature-specific neurons within human posterior auditory cortex filters superfluous sounds from entering one's awareness.</dcterms:abstract>
        <dc:date>2004-04-27</dc:date>
        <z:libraryCatalog>pnas.org (Atypon)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.pnas.org/doi/full/10.1073/pnas.0303760101</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-06 10:14:09</dcterms:dateSubmitted>
        <dc:description>Publisher: Proceedings of the National Academy of Sciences</dc:description>
        <bib:pages>6809-6814</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_610">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/610/Jääskeläinen et al. - 2004 - Human posterior auditory cortex gates novel sounds.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.pnas.org/doi/pdf/10.1073/pnas.0303760101</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-06 10:14:12</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Data rdf:about="#item_611">
        <z:itemType>computerProgram</z:itemType>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Vienna, Austria</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>R Foundation for Statistical Computing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <z:programmers>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                       <foaf:surname>R core team</foaf:surname>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </z:programmers>
        <prism:edition>3.0.2</prism:edition>
        <dc:title>R: a language and environment for statistical computing</dc:title>
        <dc:date>2019</dc:date>
    </bib:Data>
    <bib:Article rdf:about="http://www.hindawi.com/journals/bn/2015/469508/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0953-4180,%201875-8584"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Xide</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Tao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gao</foaf:surname>
                        <foaf:givenName>Dingguo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_614"/>
        <dc:title>The Mismatch Negativity: An Indicator of Perception of Regularities in Music</dc:title>
        <dcterms:abstract>This paper reviews music research using Mismatch Negativity (MMN). MMN is a deviation-specific component of auditory event-related potential (EPR), which detects a deviation between a sound and an internal representation (e.g.,
              memory trace
              ). Recent studies have expanded the notion and the paradigms of MMN to higher-order music processing such as those involving short melodies, harmony chord, and music syntax. In this vein, we firstly reviewed the evolution of MMN from sound to music and then mainly compared the differences of MMN features between musicians and nonmusicians, followed by the discussion of the potential roles of the training effect and the natural exposure in MMN. Since MMN can serve as an index of neural plasticity, it thus can be widely used in clinical and other applied areas, such as detecting music preference in newborns or assessing wholeness of central auditory system of hearing illness. Finally, we pointed out some open questions and further directions. Current music perception research using MMN has mainly focused on relatively low hierarchical structure of music perception. To fully understand the neural substrates underlying processing of regularities in music, it is important and beneficial to combine MMN with other experimental paradigms such as early right-anterior negativity (ERAN).</dcterms:abstract>
        <dc:date>2015</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>The Mismatch Negativity</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://www.hindawi.com/journals/bn/2015/469508/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-08 12:29:24</dcterms:dateSubmitted>
        <bib:pages>1-12</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0953-4180,%201875-8584">
        <prism:volume>2015</prism:volume>
        <dc:title>Behavioural Neurology</dc:title>
        <dc:identifier>DOI 10.1155/2015/469508</dc:identifier>
        <dcterms:alternative>Behavioural Neurology</dcterms:alternative>
        <dc:identifier>ISSN 0953-4180, 1875-8584</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_614">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/614/Yu et al. - 2015 - The Mismatch Negativity An Indicator of Perceptio.pdf"/>
        <dc:title>Yu et al. - 2015 - The Mismatch Negativity An Indicator of Perceptio.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://downloads.hindawi.com/journals/bn/2015/469508.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-08 12:29:20</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S1388245705002786">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1388-2457"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maekawa</foaf:surname>
                        <foaf:givenName>Toshihiko</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goto</foaf:surname>
                        <foaf:givenName>Yoshinobu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kinukawa</foaf:surname>
                        <foaf:givenName>Naoko</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Taniwaki</foaf:surname>
                        <foaf:givenName>Takayuki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kanba</foaf:surname>
                        <foaf:givenName>Shigenobu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tobimatsu</foaf:surname>
                        <foaf:givenName>Shozo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_619"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Event-related potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Pre-attentive information processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Visual mismatch negativity</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Windmill pattern stimulation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Functional characterization of mismatch negativity to a visual stimulus</dc:title>
        <dcterms:abstract>Objective
To record mismatch negativity (MMN) to a visual stimulus fulfilling similar criteria to those of auditory MMN (A-MMN).
Methods
Twelve normal adults were instructed to simultaneously listen to a story and to pay attention to a visual target. Three windmill patterns that differed in the number of vanes (standard, deviant, or target) were used as visual stimuli, and were randomly presented. To ensure endogeneity, standard and deviant stimuli were alternated. To vary differences between frequent (standard) and infrequent (deviant) stimuli, deviants were changed by modulating the number of vanes. To examine effects of physical features of the target stimulus on changes in detection, two target conditions were used. The deviant-related component (DRC) was obtained by subtracting event-related potentials (ERPs) to the deviant stimulus from those to the standard stimulus.
Results
Seven subjects completed all phases of the experiment. Behavioral performances indicated that subjects' attention was directed by auditory context and identification of the target stimulus. Visual DRC appeared 150–300ms after stimulus onset, and consisted of an early (DRN1) and a late (DRN2) component. Magnitude of deviancy from standard stimulus significantly influenced latency of DRN2 but not its magnitude, while changes in target stimulus affected latencies of both DRN1 and DRN2.
Conclusions
Our DRCs satisfied criteria for A-MMN. In contrast to A-MMN, only latency of the DRC was associated with visual sensory discrimination and attentional reorienting.
Significance
It is possible to record valid MMN to a visual stimulus, which allows the study of preattentive visual information processing.</dcterms:abstract>
        <dc:date>2005-10-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1388245705002786</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-08 12:31:08</dcterms:dateSubmitted>
        <bib:pages>2392-2402</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1388-2457">
        <prism:volume>116</prism:volume>
        <dc:title>Clinical Neurophysiology</dc:title>
        <dc:identifier>DOI 10.1016/j.clinph.2005.07.006</dc:identifier>
        <prism:number>10</prism:number>
        <dcterms:alternative>Clinical Neurophysiology</dcterms:alternative>
        <dc:identifier>ISSN 1388-2457</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_619">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/619/S1388245705002786.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1388245705002786?casa_token=kmAtt3pzLrQAAAAA:1vz-8M8APNsM3O_HE1BGnKKbcPKqwC1ZypiILjAIKMYBzJfJQ9I1hQUWsqLQCODqCpzv0EmkCXs</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-08 12:31:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1159/000013874">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1420-3030"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Näätänen</foaf:surname>
                        <foaf:givenName>Risto</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Escera</foaf:surname>
                        <foaf:givenName>Carles</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_621"/>
        <dc:title>Mismatch Negativity: Clinical and Other Applications</dc:title>
        <dcterms:abstract>The perspectives of application of the mismatch negativity (MMN), generated by the brain’s automatic response to change in auditory stimulation, are discussed. In light of the fact that the MMN (and its magnetic equivalent MMNm) currently provides the only objective measure of the accuracy of the central auditory function, these perspectives appear very promising. The MMN can be measured in the absence of attention and task requirements, which makes it particularly suitable for testing different clinical populations and infants. Furthermore, the MMN enables one to evaluate the accuracy of auditory discrimination separately for any acoustic feature, such as frequency, intensity and duration, and for learned categories, such as the phonemes of a particular language. In addition, by measuring the decay of the MMN amplitude as a function of the interstimulus interval, it is possible to estimate the duration of sensory (echoic) memory.</dcterms:abstract>
        <dc:date>2000-05-19</dc:date>
        <z:shortTitle>Mismatch Negativity</z:shortTitle>
        <z:libraryCatalog>Silverchair</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1159/000013874</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-08 12:32:02</dcterms:dateSubmitted>
        <bib:pages>105-110</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1420-3030">
        <prism:volume>5</prism:volume>
        <dc:title>Audiology and Neurotology</dc:title>
        <dc:identifier>DOI 10.1159/000013874</dc:identifier>
        <prism:number>3-4</prism:number>
        <dcterms:alternative>Audiology and Neurotology</dcterms:alternative>
        <dc:identifier>ISSN 1420-3030</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_621">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/621/Mismatch-Negativity-Clinical-and-Other.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://karger.com/aud/article-abstract/5/3-4/105/45092/Mismatch-Negativity-Clinical-and-Other?redirectedFrom=fulltext</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-08 12:32:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1007/s10548-014-0365-7">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1573-6792"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stevenson</foaf:surname>
                        <foaf:givenName>Ryan A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghose</foaf:surname>
                        <foaf:givenName>Dipanwita</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fister</foaf:surname>
                        <foaf:givenName>Juliane Krueger</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sarko</foaf:surname>
                        <foaf:givenName>Diana K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Altieri</foaf:surname>
                        <foaf:givenName>Nicholas A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nidiffer</foaf:surname>
                        <foaf:givenName>Aaron R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kurela</foaf:surname>
                        <foaf:givenName>LeAnne R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Siemann</foaf:surname>
                        <foaf:givenName>Justin K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>James</foaf:surname>
                        <foaf:givenName>Thomas W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wallace</foaf:surname>
                        <foaf:givenName>Mark T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_623"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Accuracy</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Audiovisual</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>EEG</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>fMRI</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Response time</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Sensory processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Identifying and Quantifying Multisensory Integration: A Tutorial Review</dc:title>
        <dcterms:abstract>We process information from the world through multiple senses, and the brain must decide what information belongs together and what information should be segregated. One challenge in studying such multisensory integration is how to quantify the multisensory interactions, a challenge that is amplified by the host of methods that are now used to measure neural, behavioral, and perceptual responses. Many of the measures that have been developed to quantify multisensory integration (and which have been derived from single unit analyses), have been applied to these different measures without much consideration for the nature of the process being studied. Here, we provide a review focused on the means with which experimenters quantify multisensory processes and integration across a range of commonly used experimental methodologies. We emphasize the most commonly employed measures, including single- and multiunit responses, local field potentials, functional magnetic resonance imaging, and electroencephalography, along with behavioral measures of detection, accuracy, and response times. In each section, we will discuss the different metrics commonly used to quantify multisensory interactions, including the rationale for their use, their advantages, and the drawbacks and caveats associated with them. Also discussed are possible alternatives to the most commonly used metrics.</dcterms:abstract>
        <dc:date>2014-11-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Identifying and Quantifying Multisensory Integration</z:shortTitle>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/s10548-014-0365-7</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:06:43</dcterms:dateSubmitted>
        <bib:pages>707-730</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1573-6792">
        <prism:volume>27</prism:volume>
        <dc:title>Brain Topography</dc:title>
        <dc:identifier>DOI 10.1007/s10548-014-0365-7</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Brain Topogr</dcterms:alternative>
        <dc:identifier>ISSN 1573-6792</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_623">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/623/Stevenson et al. - 2014 - Identifying and Quantifying Multisensory Integrati.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs10548-014-0365-7.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:06:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0167876011002376">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0167-8760"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bendixen</foaf:surname>
                        <foaf:givenName>Alexandra</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>SanMiguel</foaf:surname>
                        <foaf:givenName>Iria</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schröger</foaf:surname>
                        <foaf:givenName>Erich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_625"/>
        <link:link rdf:resource="#item_626"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Auditory information processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Event-related potential (ERP)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Match detection</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Mismatch detection</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Oscillatory activity</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Predictive modeling</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Early electrophysiological indicators for predictive processing in audition: A review</dc:title>
        <dcterms:abstract>The auditory system essentially deals with sequential type of input and thus requires processing that is particularly suited to extract stimulus relations within a sequence. Evidence from a variety of paradigms converges to show that the auditory system automatically uses stimulus predictability for facilitating its sequential processing. This type of predictive processing does not require attentional processing of the sounds or cognitive control of the predictions, nor does it involve the preparation of motor responses to the auditory stimuli. We will present a taxonomy of paradigms and resulting electrophysiological indicators for such automatic predictive processing in terms of event-related potential components and oscillatory activity. These indicators will include signals of fulfilled predictions (match signals such as N1 attenuation, repetition positivity, and early evoked gamma band response enhancement) as well as signals of violated predictions (mismatch signals such as the mismatch negativity and stimulus omission responses). We will show how recent approaches have revealed particularly early indicators of predictive processing down to the level of the auditory middle-latency responses. We will discuss the strength of the various indicators in terms of a truly predictive account of auditory processing (as opposed to, e.g., a retrospective verification of predictions). Finally, we will discuss the benefits of a predictive system within and beyond auditory processing. In conclusion, we argue in favor of the overwhelming evidence for predictions in audition, flexibly instantiated on different levels and timescales, and we aim to provide guidance along a variety of research paradigms illustrating the existence of these predictions.</dcterms:abstract>
        <dc:date>2012-02-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Early electrophysiological indicators for predictive processing in audition</z:shortTitle>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0167876011002376</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:26:40</dcterms:dateSubmitted>
        <bib:pages>120-131</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0167-8760">
        <dcterms:isPartOf>
            <bib:Series>
                <dc:title>Predictive information processing in the brain: Principles, neural mechanisms and models</dc:title>
            </bib:Series>
        </dcterms:isPartOf>
        <prism:volume>83</prism:volume>
        <dc:title>International Journal of Psychophysiology</dc:title>
        <dc:identifier>DOI 10.1016/j.ijpsycho.2011.08.003</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>International Journal of Psychophysiology</dcterms:alternative>
        <dc:identifier>ISSN 0167-8760</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_625">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/625/Bendixen et al. - 2012 - Early electrophysiological indicators for predicti.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0167876011002376/pdfft?md5=0a20fc393119186d5a8aa203684eea11&amp;pid=1-s2.0-S0167876011002376-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:26:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_626">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/626/S0167876011002376.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0167876011002376</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:26:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/0001691878900069">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0001-6918"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Näätänen</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gaillard</foaf:surname>
                        <foaf:givenName>A. W. K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mäntysalo</foaf:surname>
                        <foaf:givenName>S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_628"/>
        <link:link rdf:resource="#item_629"/>
        <dc:title>Early selective-attention effect on evoked potential reinterpreted</dc:title>
        <dcterms:abstract>In a dichotic listening situation stimuli were presented one at a time and at random to either ear of the subject at constant inter-stimulus intervals of 800 msec. The subject's task was to detect and count occasional slightly different stimuli in one ear. In Experiment 1, these ‘signal’ stimuli were slightly louder, and in Experiment 2 they had a slightly higher pitch, than the much more frequent, ‘standard’, stimuli. In both experiments signals occured randomly at either ear. Separate evoked potentials from three different locations were recorded for each of the four kinds of stimuli (attended signals, unattended signals, attended standards, unattended standards). Contrary to Hillyard et al. (1973), no early (N1 component) evoked-potential enhancement was observed to stimuli to the attended ear as compared with those to the unattended ear, but there was a later negative shift superimposed on potentials elicited by the former stimuli. This negative shift was considered identical to the N1 enhancement of Hillyard and his colleagues which in the present study was forced, by the longer inter-stimulus interval used, to demonstrate temporal dissociation with the N1 component. The ‘Hillyard effect’ was, consequently, explained as being caused by a superimposition of a CNV kind of negative shift on the evoked potential to the attended stimuli rather than by a growth of the ‘real’ N1 component of the evoked potential.</dcterms:abstract>
        <dc:date>1978-07-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/0001691878900069</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:36:26</dcterms:dateSubmitted>
        <bib:pages>313-329</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0001-6918">
        <prism:volume>42</prism:volume>
        <dc:title>Acta Psychologica</dc:title>
        <dc:identifier>DOI 10.1016/0001-6918(78)90006-9</dc:identifier>
        <prism:number>4</prism:number>
        <dcterms:alternative>Acta Psychologica</dcterms:alternative>
        <dc:identifier>ISSN 0001-6918</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_628">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/628/Näätänen et al. - 1978 - Early selective-attention effect on evoked potenti.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/0001691878900069/pdf?md5=ef2eb116b72d6585c8f5c3b8ba3387b8&amp;pid=1-s2.0-0001691878900069-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:36:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_629">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/629/0001691878900069.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/0001691878900069</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:36:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://www.biorxiv.org/content/10.1101/2022.10.27.514010v1">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>bioRxiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Grundei</foaf:surname>
                        <foaf:givenName>Miro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schröder</foaf:surname>
                        <foaf:givenName>Pia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gijsen</foaf:surname>
                        <foaf:givenName>Sam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Blankenburg</foaf:surname>
                        <foaf:givenName>Felix</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_637"/>
        <dc:title>EEG mismatch responses in a multi-modal roving stimulus paradigm provide evidence for probabilistic inference across audition, somatosensation and vision</dc:title>
        <dcterms:abstract>The human brain is constantly subjected to a multi-modal stream of probabilistic sensory inputs. EEG signatures, such as the mismatch negativity (MMN) and the P3, can give valuable insight into neuronal probabilistic inference. Although reported for different modalities, mismatch responses have largely been studied in isolation, with a strong focus on the auditory MMN. To investigate the extent to which early and late mismatch responses across modalities represent comparable signatures of uni- and cross-modal probabilistic inference in the hierarchically structured cortex, we recorded EEG from 32 participants undergoing a novel tri-modal roving stimulus paradigm. The employed sequences consisted of high and low intensity stimuli in the auditory, somatosensory and visual modalities and were governed by uni-modal transition probabilities and cross-modal conditional dependencies. We found modality specific signatures of MMN (∼100-200ms) in all three modalities, which were source localized to the respective sensory cortices and shared right lateralized pre-frontal sources. Additionally, we identified a cross-modal signature of mismatch processing in the P3a time range (∼300-350ms), for which a common network with frontal dominance was found. Across modalities, the mismatch responses showed highly comparable parametric effects of stimulus train length, which were driven by standard and deviant response modulations in opposite directions. Strikingly, the P3a responses across modalities were increased for mispredicted compared to predicted and unpredictable stimuli, suggesting sensitivity to cross-modal predictive information. Finally, model comparisons indicated that the observed single trial dynamics were best captured by Bayesian learning models tracking uni-modal stimulus transitions as well as cross-modal conditional dependencies.</dcterms:abstract>
        <dc:date>2022-10-28</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>bioRxiv</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.biorxiv.org/content/10.1101/2022.10.27.514010v1</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:46:40</dcterms:dateSubmitted>
        <dc:rights>© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
        <dc:description>Pages: 2022.10.27.514010
Section: New Results</dc:description>
        <dc:identifier>DOI 10.1101/2022.10.27.514010</dc:identifier>
    </rdf:Description>
    <z:Attachment rdf:about="#item_637">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/637/Grundei et al. - 2022 - EEG mismatch responses in a multi-modal roving sti.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.biorxiv.org/content/biorxiv/early/2022/10/28/2022.10.27.514010.full.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:46:45</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://econtent.hogrefe.com/doi/abs/10.1027/0269-8803.21.34.147">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0269-8803"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winkler</foaf:surname>
                        <foaf:givenName>István</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Auditory deviance detection</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>auditory memory</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>auditory scene analysis</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>MMN</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>predictive model</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>regularity extraction</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>temporal grouping</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Interpreting the Mismatch Negativity</dc:title>
        <dcterms:abstract>The widely accepted “memory-mismatch” interpretation of the mismatch negativity (MMN) event-related brain potential (ERP) suggests that an MMN is elicited when an acoustic event deviates from a memory record describing the immediate history of the sound sequence. The first variant of the memory-mismatch theory suggested that the memory underlying MMN generation was a strong auditory sensory memory trace, which encoded the repetitive standard sound. This “trace-mismatch” explanation of MMN has been primarily based on results obtained in the auditory oddball paradigm. However, in recent years, MMN has been observed in stimulus paradigms containing no frequently repeating sound. We now suggest a different variant of the memory-mismatch interpretation of MMN in order to provide a unified explanation of all MMN phenomena. The regularity-violation explanation of MMN assumes that the memory records retaining the history of auditory stimulation are regularity representations. These representations encode rules extracted from the regular intersound relationships, which are mapped to the concrete sound sequence by finely detailed auditory sensory information. Auditory events are compared with temporally aligned predictions drawn from the regularity representations (predictive models) and the observable MMN response reflects a process updating the representations of those detected regularities whose prediction was mismatched by the acoustic input. It is further suggested that the auditory deviance detection system serves to organize sound in the brain: The predictive models maintained by the MMN-generating process provide the basis of temporal grouping, a crucial step in the formation of auditory objects.</dcterms:abstract>
        <dc:date>2007-01</dc:date>
        <z:libraryCatalog>econtent.hogrefe.com (Atypon)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://econtent.hogrefe.com/doi/abs/10.1027/0269-8803.21.34.147</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 10:54:24</dcterms:dateSubmitted>
        <dc:description>Publisher: Hogrefe Publishing</dc:description>
        <bib:pages>147-163</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0269-8803">
        <prism:volume>21</prism:volume>
        <dc:title>Journal of Psychophysiology</dc:title>
        <dc:identifier>DOI 10.1027/0269-8803.21.34.147</dc:identifier>
        <prism:number>3-4</prism:number>
        <dc:identifier>ISSN 0269-8803</dc:identifier>
    </bib:Journal>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S1388245707001939">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>118</prism:volume>
                <dc:title>Clinical Neurophysiology</dc:title>
                <dc:identifier>DOI 10.1016/j.clinph.2007.04.026</dc:identifier>
                <prism:number>12</prism:number>
                <dcterms:alternative>Clinical Neurophysiology</dcterms:alternative>
                <dc:identifier>ISSN 1388-2457</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Näätänen</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paavilainen</foaf:surname>
                        <foaf:givenName>P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rinne</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Alho</foaf:surname>
                        <foaf:givenName>K.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_640"/>
        <link:link rdf:resource="#item_641"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Auditory cortex</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Central auditory processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Event-related potential (ERP)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Mismatch negativity (MMN)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>The mismatch negativity (MMN) in basic research of central auditory processing: A review</dc:title>
        <dcterms:abstract>In the present article, the basic research using the mismatch negativity (MMN) and analogous results obtained by using the magnetoencephalography (MEG) and other brain-imaging technologies is reviewed. This response is elicited by any discriminable change in auditory stimulation but recent studies extended the notion of the MMN even to higher-order cognitive processes such as those involving grammar and semantic meaning. Moreover, MMN data also show the presence of automatic intelligent processes such as stimulus anticipation at the level of auditory cortex. In addition, the MMN enables one to establish the brain processes underlying the initiation of attention switch to, conscious perception of, sound change in an unattended stimulus stream.</dcterms:abstract>
        <dc:date>2007-12-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>The mismatch negativity (MMN) in basic research of central auditory processing</z:shortTitle>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1388245707001939</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 11:03:43</dcterms:dateSubmitted>
        <bib:pages>2544-2590</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_640">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/640/Näätänen et al. - 2007 - The mismatch negativity (MMN) in basic research of.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1388245707001939/pdfft?md5=50bcc70cc58abc84a48b1045c0c7aa03&amp;pid=1-s2.0-S1388245707001939-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 11:03:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_641">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/641/S1388245707001939.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1388245707001939</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 11:04:12</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Book rdf:about="urn:isbn:978-0-19-505038-7">
        <z:itemType>book</z:itemType>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Oxford University Press</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nunez</foaf:surname>
                        <foaf:givenName>Paul L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Srinivasan</foaf:surname>
                        <foaf:givenName>Ramesh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_643"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Medical / Neurology</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Medical / Neuroscience</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Electric Fields of the Brain: The Neurophysics of EEG</dc:title>
        <dcterms:abstract>Electroencephalography (EEG) is practiced by neurologists, cognitive neuroscientists, and others interested in functional brain imaging. Whether for clinical or experimental purposes, all studies share a common purpose-to relate scalp potentials to the underlying neurophysiology. Electrical potentials on the scalp exhibit spatial and temporal patterns that depend on the nature and location of the sources and the way that currents and fields spread through tissue. Because these dynamic patterns are correlated with behavior and cognition, EEG provides a &quot;window on the mind,&quot; correlating physiology and psychology. This classic and widely acclaimed text, originally published in 1981, filled the large gap between EEG and the physical sciences. It has now been brought completely up to date and will again serve as an invaluable resource for understanding the principles of electric fields in living tissue and for using hard science to study human consciousness and cognition. No comparable volume exists for it is no easy task to explain the problems of EEG in clear language, with mathematics presented mainly in appendices. Among the many topics covered by the Second Edition are micro and meso (intermediate scale) synaptic sources, electrode placement, choice of reference, volume conduction, power and coherence measures, projection of scalp potentials to dura surface, dynamic signatures of conscious experience, neural networks immersed in global fields of synaptic action, and physiological bases for brain source dynamics. The Second Edition is an invaluable resource for neurologists, neuroscientists (especially cognitive neuroscientists), biomedical engineers, and their students and trainees. It will also appeal to physicists, mathematicians, computer scientists, psychiatrists, and industrial engineers interested in EEG.</dcterms:abstract>
        <dc:date>2006</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Electric Fields of the Brain</z:shortTitle>
        <z:libraryCatalog>Google Books</z:libraryCatalog>
        <dc:description>Google-Books-ID: fUv54as56_8C</dc:description>
        <dc:identifier>ISBN 978-0-19-505038-7</dc:identifier>
        <z:numPages>629</z:numPages>
    </bib:Book>
    <z:Attachment rdf:about="#item_643">
        <z:itemType>attachment</z:itemType>
        <dc:title>Google Books Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://books.google.dk/books?id=fUv54as56_8C</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-10 13:44:21</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0165027004002511">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0165-0270"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leonowicz</foaf:surname>
                        <foaf:givenName>Zbigniew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Karvanen</foaf:surname>
                        <foaf:givenName>Juha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shishkin</foaf:surname>
                        <foaf:givenName>Sergei L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_646"/>
        <link:link rdf:resource="#item_647"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Averaging</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Event-related potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Evoked potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Mean</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Median</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Robust estimators of location</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Trimmed estimators</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Trimmed mean</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Trimmed estimators for robust averaging of event-related potentials</dc:title>
        <dcterms:abstract>Averaging (in statistical terms, estimation of the location of data) is one of the most commonly used procedures in neuroscience and the basic procedure for obtaining event-related potentials (ERP). Only the arithmetic mean is routinely used in the current practice of ERP research, though its sensitivity to outliers is well-known. Weighted averaging is sometimes used as a more robust procedure, however, it can be not sufficiently appropriate when the signal is nonstationary within a trial. Trimmed estimators provide an alternative way to average data. In this paper, a number of such location estimators (trimmed mean, Winsorized mean and recently introduced trimmed L-mean) are reviewed, as well as arithmetic mean and median. A new robust location estimator tanh, which allows the data-dependent optimization, is proposed for averaging of small number of trials. The possibilities to improve signal-to-noise ratio (SNR) of averaged waveforms using trimmed location estimators are demonstrated for epochs randomly drawn from a set of real auditory evoked potential data.</dcterms:abstract>
        <dc:date>2005-03-15</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0165027004002511</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-10 14:14:10</dcterms:dateSubmitted>
        <bib:pages>17-26</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0165-0270">
        <prism:volume>142</prism:volume>
        <dc:title>Journal of Neuroscience Methods</dc:title>
        <dc:identifier>DOI 10.1016/j.jneumeth.2004.07.008</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Journal of Neuroscience Methods</dcterms:alternative>
        <dc:identifier>ISSN 0165-0270</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_646">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/646/Leonowicz et al. - 2005 - Trimmed estimators for robust averaging of event-r.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0165027004002511/pdfft?md5=043a1db732a17338c336b2b7c9d08be0&amp;pid=1-s2.0-S0165027004002511-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-10 14:14:13</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_647">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/647/S0165027004002511.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0165027004002511?casa_token=ZUKqkcdydasAAAAA:EQ8MCV6JBiTHoC0dCKKIoab43mP4EjZHE0ofr-3IFbhFRPiypUwKEPN_q6DJPz937TilWfAVb70</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-10 14:14:18</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://journals.lww.com/neuroreport/Fulltext/1997/05260/Temporal_window_of_integration_revealed_by_MMN_to.35.aspx?casa_token=JivaaV-fdjgAAAAA:NQgDUxcJwuLN670cFiArtyMTuS5oN6O2FJkcKAvZRvOpYWRNy7gDLYbsPlVytpYIIae_gp63ygLRxtejyq01Ag">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0959-4965"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yabe</foaf:surname>
                        <foaf:givenName>Hirooki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tervaniemi</foaf:surname>
                        <foaf:givenName>Mari</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reinikainen</foaf:surname>
                        <foaf:givenName>Kalevi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Näätänen</foaf:surname>
                        <foaf:givenName>Risto</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_653"/>
        <dc:title>Temporal window of integration revealed by MMN to sound omission</dc:title>
        <dcterms:abstract>THE central auditory system for event perception involves the integrating mechanism of sequential information addressed by the present study. The mismatch negativity (MMN) component of the event-related potentials (ERP) reflects the automatic detection of sound change. ERPs to occasionally omitted stimuli were measured when sequences with constant stimulus onset asynchronies (SOAs) were presented. In separate blocks, the SOA was from 100 to 350 ms. A clear MMN was elicited by a stimulus omission in a sequence of regularly spaced tone pips only when the SOA was shorter than 150 ms, yielding an estimate for the duration of the temporal window of integration used the perceptual segregation of auditory events.</dcterms:abstract>
        <dc:date>May 26, 1997</dc:date>
        <z:language>en-US</z:language>
        <z:libraryCatalog>journals.lww.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.lww.com/neuroreport/Fulltext/1997/05260/Temporal_window_of_integration_revealed_by_MMN_to.35.aspx?casa_token=JivaaV-fdjgAAAAA:NQgDUxcJwuLN670cFiArtyMTuS5oN6O2FJkcKAvZRvOpYWRNy7gDLYbsPlVytpYIIae_gp63ygLRxtejyq01Ag</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:39:47</dcterms:dateSubmitted>
        <bib:pages>1971</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0959-4965">
        <prism:volume>8</prism:volume>
        <dc:title>NeuroReport</dc:title>
        <prism:number>8</prism:number>
        <dc:identifier>ISSN 0959-4965</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_653">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/653/Temporal_window_of_integration_revealed_by_MMN_to.35.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.lww.com/neuroreport/Fulltext/1997/05260/Temporal_window_of_integration_revealed_by_MMN_to.35.aspx?casa_token=JivaaV-fdjgAAAAA:NQgDUxcJwuLN670cFiArtyMTuS5oN6O2FJkcKAvZRvOpYWRNy7gDLYbsPlVytpYIIae_gp63ygLRxtejyq01Ag</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:39:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S1053811901907669">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1053-8119"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hughes</foaf:surname>
                        <foaf:givenName>H. C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Darcey</foaf:surname>
                        <foaf:givenName>T. M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Barkan</foaf:surname>
                        <foaf:givenName>H. I.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Williamson</foaf:surname>
                        <foaf:givenName>P. D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roberts</foaf:surname>
                        <foaf:givenName>D. W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Aslin</foaf:surname>
                        <foaf:givenName>C. H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_657"/>
        <link:link rdf:resource="#item_658"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>auditory evoked potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>auditory oddball</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>auditory stimuli</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>event-related potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>evoked potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>human</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>intracerebral potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>intracranial recording</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>mismatch negativity</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>omitted stimuli</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>stimulus deviance</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>temporal cortex</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Responses of Human Auditory Association Cortex to the Omission of an Expected Acoustic Event</dc:title>
        <dcterms:abstract>Unexpected auditory events initiate a complex set of event-related potentials (ERPs) that vary in their latency and anatomical localization. Such “mismatch” responses include active responses to the omission of an expected event or the omission of elements in expected stimulus composites. Here we describe intracranial recordings of middle-latency ERPs elicited by the omission of an auditory event. We first presented a sequence of tones at regular temporal intervals and the tone was omitted 20% of the time. In a second condition, we presented a sequence of tone pairs and the second tone of the pair was omitted 20% of the time. These two conditions are complementary in that the single tone conformed to the expectancy in one condition, but violated the expectancy in the other. All patients demonstrated localized cortical responses to missing tones that were topographically similar to the responses evoked by actual tones. Responses to both actual and omitted tones were observed bilaterally in the vicinity of the temporal–parietal junction, where we also obtained midlatency ERPs to a variety of other auditory stimuli. Responses that appeared to be selective for the nonoccurrence of expected tones were also observed in a number of subjects. We interpret these effects in terms of processes associated with the comparison of sensory inputs to the contents of a short-term auditory memory. Such a system could automatically detect deviant auditory events, and provide input to higher-level, task-dependent cognitive processes.</dcterms:abstract>
        <dc:date>2001-06-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1053811901907669</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:40:56</dcterms:dateSubmitted>
        <bib:pages>1073-1089</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1053-8119">
        <prism:volume>13</prism:volume>
        <dc:title>NeuroImage</dc:title>
        <dc:identifier>DOI 10.1006/nimg.2001.0766</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>NeuroImage</dcterms:alternative>
        <dc:identifier>ISSN 1053-8119</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_657">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/657/Hughes et al. - 2001 - Responses of Human Auditory Association Cortex to .pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1053811901907669/pdf?md5=a3309f5ba823a5652cb656fd926e8416&amp;pid=1-s2.0-S1053811901907669-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:41:00</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_658">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/658/S1053811901907669.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S1053811901907669</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:41:02</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2011.01336.x">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1469-8986"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salisbury</foaf:surname>
                        <foaf:givenName>Dean F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_660"/>
        <link:link rdf:resource="#item_661"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Emitted potentials</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>ERP</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>MMN</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Sensory memory</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Finding the missing stimulus mismatch negativity (MMN): Emitted MMN to violations of an auditory gestalt</dc:title>
        <dcterms:abstract>Deviations from repetitive auditory stimuli evoke a mismatch negativity (MMN). Counterintuitively, omissions of repetitive stimuli do not. Violations of patterns reflecting complex rules also evoke MMN. To detect a MMN to missing stimuli, we developed an auditory gestalt task using one stimulus. Groups of six pips (50 ms duration, 330 ms stimulus onset asynchrony [SOA], 400 trials), were presented with an intertrial interval (ITI) of 750 ms while subjects (n = 16) watched a silent video. Occasional deviant groups had missing 4th or 6th tones (50 trials each). Missing stimuli evoked a MMN (p &lt; .05). The missing 4th (−0.8 µV, p &lt;.01) and the missing 6th stimuli (−1.1 µV, p &lt; .05) were more negative than standard 6th stimuli (0.3 µV). MMN can be elicited by a missing stimulus at long SOAs by violation of a gestalt grouping rule. Patterned stimuli appear more sensitive to omissions and ITI than homogenous streams.</dcterms:abstract>
        <dc:date>2012</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Finding the missing stimulus mismatch negativity (MMN)</z:shortTitle>
        <z:libraryCatalog>Wiley Online Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2011.01336.x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:41:40</dcterms:dateSubmitted>
        <dc:rights>Copyright © 2012 Society for Psychophysiological Research</dc:rights>
        <dc:description>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2011.01336.x</dc:description>
        <bib:pages>544-548</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1469-8986">
        <prism:volume>49</prism:volume>
        <dc:title>Psychophysiology</dc:title>
        <dc:identifier>DOI 10.1111/j.1469-8986.2011.01336.x</dc:identifier>
        <prism:number>4</prism:number>
        <dc:identifier>ISSN 1469-8986</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_660">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/660/Salisbury - 2012 - Finding the missing stimulus mismatch negativity (.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1469-8986.2011.01336.x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:41:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_661">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/661/j.1469-8986.2011.01336.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/full/10.1111/j.1469-8986.2011.01336.x?casa_token=yXbEUbzDRfIAAAAA%3AMLroVIXcLHQKA5redRQntWAUjGukJPc8tWn7AqcWkR22n3oMBRhbupHUfK4lD65xLOeDaOa-rU49ZyBX</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:41:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0167876013000652">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>88</prism:volume>
                <dc:title>International Journal of Psychophysiology</dc:title>
                <dc:identifier>DOI 10.1016/j.ijpsycho.2013.03.015</dc:identifier>
                <prism:number>2</prism:number>
                <dcterms:alternative>International Journal of Psychophysiology</dcterms:alternative>
                <dc:identifier>ISSN 0167-8760</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Paavilainen</foaf:surname>
                        <foaf:givenName>Petri</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_668"/>
        <link:link rdf:resource="#item_669"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Auditory information processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>ERP (event-related potential)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Implicit cognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>MMN (mismatch negativity)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Predictive processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>The mismatch-negativity (MMN) component of the auditory event-related potential to violations of abstract regularities: A review</dc:title>
        <dcterms:abstract>The mismatch-negativity (MMN) component of the event-related potential (ERP) has been extensively used to study the preattentive processing and storage of regularities in basic physical stimulus features (e.g., frequency, intensity, spatial location). However, studies reviewed in the present article reveal that the auditory analysis reflected by MMN also includes the detection and use of more complex, “abstract”, regularities based, for example, on relationships between various physical features of the stimuli or in patterns present in the auditory stream. When these regularities are violated, then MMN is elicited. Thus, the central auditory system performs even at the pre-attentive, auditory-cortex level surprisingly “cognitive” operations, such as generalization leading to simple concept formation, rule extraction and prediction of future stimuli. The information extracted often seems to be in an implicit form, not directly available to conscious processes and difficult to express verbally. It can nevertheless influence the behavior of the subject, for example, the regularity violations can temporarily impair performance in the primary task. Neural, behavioral and cognitive events associated with the development of the regularity representations are discussed.</dcterms:abstract>
        <dc:date>2013-05-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>The mismatch-negativity (MMN) component of the auditory event-related potential to violations of abstract regularities</z:shortTitle>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0167876013000652</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:51:00</dcterms:dateSubmitted>
        <bib:pages>109-123</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_668">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/668/Paavilainen - 2013 - The mismatch-negativity (MMN) component of the aud.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0167876013000652/pdfft?md5=a4468971beaf20de2f1331acb4f70059&amp;pid=1-s2.0-S0167876013000652-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:51:02</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_669">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/669/S0167876013000652.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0167876013000652?casa_token=7zd_4UEsZ1UAAAAA:uBD9HqvV8weL7zNEYIJ22gwcBvCJyT9-gS4T_4Y9Ek4CQJNNstISP_RuUKFpzjxWW_TnHuvarJ0</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 07:51:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0006899306013898">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0006-8993"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Czigler</foaf:surname>
                        <foaf:givenName>István</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winkler</foaf:surname>
                        <foaf:givenName>István</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pató</foaf:surname>
                        <foaf:givenName>Lívia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Várnagy</foaf:surname>
                        <foaf:givenName>Anna</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weisz</foaf:surname>
                        <foaf:givenName>Júlia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Balázs</foaf:surname>
                        <foaf:givenName>László</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_671"/>
        <link:link rdf:resource="#item_672"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Event-related potential</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Stimulus omission</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Temporal window of integration</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Visual mismatch negativity</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Visual temporal window of integration as revealed by the visual mismatch negativity event-related potential to stimulus omissions</dc:title>
        <dcterms:abstract>We studied whether, similarly to the auditory modality, short-period temporal integration processes occur in vision. Event-related potentials (ERP) were recorded for occasional stimulus omissions from sequences of patterned visual stimuli. A posterior negative component emerged only when the constant stimulus onset asynchrony (SOA) was shorter than 150 ms. This upper limit is comparable with the duration of the temporal window of integration observed in the auditory modality (including experiments studying the effects of stimulus omissions). Parameters of the posterior negativity were highly similar irrespective of whether the stimuli were task-relevant or not (Experiment 1). Thus, we identified this potential as the visual mismatch negativity (vMMN) component, which reflects task-independent detection of violating regularities of the stimulation. vMMN was followed by an anterior positivity (the P3a), indicating attentional shifts induced by the stimulus omissions. In Experiment 2, a posterior negativity similar to that observed in Experiment 1 emerged after the termination of short trains of stimuli, again only when the SOA was shorter than 150 ms. These results support the notion of a temporal integration window in the visual modality, the duration of which is between 150 and 180 ms.</dcterms:abstract>
        <dc:date>2006-08-09</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0006899306013898</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 08:01:53</dcterms:dateSubmitted>
        <bib:pages>129-140</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0006-8993">
        <prism:volume>1104</prism:volume>
        <dc:title>Brain Research</dc:title>
        <dc:identifier>DOI 10.1016/j.brainres.2006.05.034</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Brain Research</dcterms:alternative>
        <dc:identifier>ISSN 0006-8993</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_671">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/671/Czigler et al. - 2006 - Visual temporal window of integration as revealed .pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0006899306013898/pdfft?md5=73aaedd99374e86cc8a8ee70cfb08e49&amp;pid=1-s2.0-S0006899306013898-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 08:01:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_672">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/672/S0006899306013898.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0006899306013898?casa_token=nRP5rEO5dIAAAAAA:e6mip8OjCQxDQvZS_YJ91DmYZS24g_G6UWK8agXZ4I5EAGCfv0PIgDA9_lBGV5eWqF2I_H_0eL0</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 08:02:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1162/jocn_a_00562">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0898-929X"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kok</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Failing</foaf:surname>
                        <foaf:givenName>Michel F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>de Lange</foaf:surname>
                        <foaf:givenName>Floris P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_674"/>
        <dc:title>Prior Expectations Evoke Stimulus Templates in the Primary Visual Cortex</dc:title>
        <dcterms:abstract>Sensory processing is strongly influenced by prior expectations. Valid expectations have been shown to lead to improvements in perception as well as in the quality of sensory representations in primary visual cortex. However, very little is known about the neural correlates of the expectations themselves. Previous studies have demonstrated increased activity in sensory cortex following the omission of an expected stimulus, yet it is unclear whether this increased activity constitutes a general surprise signal or rather has representational content. One intriguing possibility is that top–down expectation leads to the formation of a template of the expected stimulus in visual cortex, which can then be compared with subsequent bottom–up input. To test this hypothesis, we used fMRI to noninvasively measure neural activity patterns in early visual cortex of human participants during expected but omitted visual stimuli. Our results show that prior expectation of a specific visual stimulus evokes a feature-specific pattern of activity in the primary visual cortex (V1) similar to that evoked by the corresponding actual stimulus. These results are in line with the notion that prior expectation triggers the formation of specific stimulus templates to efficiently process expected sensory inputs.</dcterms:abstract>
        <dc:date>2014-07-01</dc:date>
        <z:libraryCatalog>Silverchair</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1162/jocn_a_00562</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 08:02:16</dcterms:dateSubmitted>
        <bib:pages>1546-1554</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0898-929X">
        <prism:volume>26</prism:volume>
        <dc:title>Journal of Cognitive Neuroscience</dc:title>
        <dc:identifier>DOI 10.1162/jocn_a_00562</dc:identifier>
        <prism:number>7</prism:number>
        <dcterms:alternative>Journal of Cognitive Neuroscience</dcterms:alternative>
        <dc:identifier>ISSN 0898-929X</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_674">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/674/Prior-Expectations-Evoke-Stimulus-Templates-in-the.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://direct.mit.edu/jocn/article-abstract/26/7/1546/28128/Prior-Expectations-Evoke-Stimulus-Templates-in-the</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 08:02:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://journals.lww.com/neuroreport/fulltext/2005/09080/organization_of_sequential_sounds_in_auditory.22.aspx?casa_token=hy0gOOi6RhUAAAAA:skB6xxqvRA2HKD7-v14wC2XrZaS_Z7UAFDUnbqNyllHnfdO4_4tJdpXqSduAuGXCp5SYYA1IPcQJRR6iJCloaQ">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>16</prism:volume>
                <dc:title>NeuroReport</dc:title>
                <dc:identifier>DOI 10.1097/01.wnr.0000177002.35193.4c</dc:identifier>
                <prism:number>13</prism:number>
                <dc:identifier>ISSN 0959-4965</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sussman</foaf:surname>
                        <foaf:givenName>Elyse S.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gumenyuk</foaf:surname>
                        <foaf:givenName>Valentina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_676"/>
        <dc:title>Organization of sequential sounds in auditory memory</dc:title>
        <dcterms:abstract>A repeating five-tone pattern was presented at several stimulus rates (200, 400, 600, and 800 ms onset-to-onset) to determine at what temporal proximity the five-tone repeating unit would be represented in memory. The mismatch negativity component of event-related brain potentials was used to index how the sounds were organized in memory when participants had no task with the sounds. Only at the 200-ms onset-to-onset pace was the five-tone sequence unitized in memory. At presentation rates of 400 ms and above, the regularity (a different frequency tone occurred every fifth tone) was not detected and mismatch negativity was elicited by these tones in the sequence. The results show that temporal proximity plays a role in unitizing successive sounds in auditory memory. These results also suggest that global relationships between successive sounds are represented at the level of auditory cortices.</dcterms:abstract>
        <dc:date>September 8, 2005</dc:date>
        <z:language>en-US</z:language>
        <z:libraryCatalog>journals.lww.com</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.lww.com/neuroreport/fulltext/2005/09080/organization_of_sequential_sounds_in_auditory.22.aspx?casa_token=hy0gOOi6RhUAAAAA:skB6xxqvRA2HKD7-v14wC2XrZaS_Z7UAFDUnbqNyllHnfdO4_4tJdpXqSduAuGXCp5SYYA1IPcQJRR6iJCloaQ</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 10:46:58</dcterms:dateSubmitted>
        <bib:pages>1519</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_676">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/676/organization_of_sequential_sounds_in_auditory.22.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.lww.com/neuroreport/fulltext/2005/09080/organization_of_sequential_sounds_in_auditory.22.aspx?casa_token=hy0gOOi6RhUAAAAA:skB6xxqvRA2HKD7-v14wC2XrZaS_Z7UAFDUnbqNyllHnfdO4_4tJdpXqSduAuGXCp5SYYA1IPcQJRR6iJCloaQ</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 10:47:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0167876015300313">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>98</prism:volume>
                <dc:title>International Journal of Psychophysiology</dc:title>
                <dc:identifier>DOI 10.1016/j.ijpsycho.2015.09.006</dc:identifier>
                <prism:number>2, Part 1</prism:number>
                <dcterms:alternative>International Journal of Psychophysiology</dcterms:alternative>
                <dc:identifier>ISSN 0167-8760</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Max</foaf:surname>
                        <foaf:givenName>Caroline</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Widmann</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schröger</foaf:surname>
                        <foaf:givenName>Erich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sussman</foaf:surname>
                        <foaf:givenName>Elyse</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_681"/>
        <link:link rdf:resource="#item_680"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Attention</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Distraction</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Explicit knowledge</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Implicit knowledge</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Mismatch negativity (MMN)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>P3a</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Predictability</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Effects of explicit knowledge and predictability on auditory distraction and target performance</dc:title>
        <dcterms:abstract>This study tested effects of task requirements and knowledge on auditory distraction effects. This was done by comparing the response to a pitch change (an irrelevant, distracting tone feature) that occurred predictably in a tone sequence (every 5th tone) under different task conditions. The same regular sound sequence was presented with task conditions varying in what information the participant was given about the predictability of the pitch change, and when this information was relevant for the task to be performed. In all conditions, participants performed a tone duration judgment task. Behavioral and event-related brain potential (ERP) measures were obtained to measure distraction effects and deviance detection. Predictable deviants produced behavioral distraction effects in all conditions. However, the P3a amplitude evoked by the predictable pitch change was largest when participants were uninformed about the regular structure of the sound sequence, showing an effect of knowledge on involuntary orienting of attention. In contrast, the mismatch negativity (MMN) component was only modulated when the regularity was relevant for the task and not by stimulus predictability itself. P3a and behavioral indices of distraction were not fully concordant. Overall, our results show differential effects of knowledge and predictability on auditory distraction effects indexed by neurophysiological (P3a) and behavioral measures.</dcterms:abstract>
        <dc:date>2015-11-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0167876015300313</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 10:49:08</dcterms:dateSubmitted>
        <bib:pages>174-181</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_681">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/681/Max et al. - 2015 - Effects of explicit knowledge and predictability o.pdf"/>
        <dc:title>Accepted Version</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://europepmc.org/articles/pmc4658268?pdf=render</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 10:49:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_680">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/680/S0167876015300313.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0167876015300313?casa_token=CqPJoWFfHm4AAAAA:0X9FGk-8awP5RUsF4JxzOSRyjDST5W_4nZ-JQi9AAKwSArIkMR_wXOTSt9vZBrngmGMCsMr65nk</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 10:49:15</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1007/s00221-014-4141-4">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1432-1106"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Chen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Valentini</foaf:surname>
                        <foaf:givenName>Elia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hu</foaf:surname>
                        <foaf:givenName>Li</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_685"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Crossmodal mismatch response</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Deviance detection</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Event-related potentials (ERPs)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Roving paradigm</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Standard formation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Functional features of crossmodal mismatch responses</dc:title>
        <dcterms:abstract>Research on brain mechanisms of deviance detection and sensory memory trace formation, best indexed by the mismatch negativity, mainly relied on the investigation of responses elicited by auditory stimuli. However, comparable less research reported the mismatch negativity elicited by somatosensory stimuli. More importantly, little is known on the functional features of mismatch deviant and standard responses across different sensory modalities. To directly compare different sensory modalities, we adopted a crossmodal roving paradigm and collected event-related potentials elicited by auditory, non-nociceptive somatosensory, and nociceptive trains of stimuli, during Active and Passive attentional conditions. We applied a topographical segmentation analysis to cluster successive scalp topographies with quasi-stable landscape of significant differences to extract crossmodal mismatch responses. We obtained three main findings. First, across different sensory modalities and attentional conditions, the formation of a standard sensory trace became robust mainly after the second stimulus repetition. Second, the neural representation of a modality deviant stimulus was influenced by the preceding sensory modality. Third, the mismatch negativity significantly covaried between Active and Passive attentional conditions within the same sensory modality, but not between different sensory modalities. These findings provide robust evidence that, while different modalities share a similar process of standard trace formation, the process of deviance detection is largely modality dependent.</dcterms:abstract>
        <dc:date>2015-02-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/s00221-014-4141-4</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 10:57:23</dcterms:dateSubmitted>
        <bib:pages>617-629</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1432-1106">
        <prism:volume>233</prism:volume>
        <dc:title>Experimental Brain Research</dc:title>
        <dc:identifier>DOI 10.1007/s00221-014-4141-4</dc:identifier>
        <prism:number>2</prism:number>
        <dcterms:alternative>Exp Brain Res</dcterms:alternative>
        <dc:identifier>ISSN 1432-1106</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_685">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/685/Zhao et al. - 2015 - Functional features of crossmodal mismatch respons.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs00221-014-4141-4.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 10:57:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/S0361923008000129">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0361-9230"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bresciani</foaf:surname>
                        <foaf:givenName>Jean-Pierre</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dammeier</foaf:surname>
                        <foaf:givenName>Franziska</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ernst</foaf:surname>
                        <foaf:givenName>Marc O.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_687"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Audition</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Multimodal integration</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Touch</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Virtual environments</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Vision</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Tri-modal integration of visual, tactile and auditory signals for the perception of sequences of events</dc:title>
        <dcterms:abstract>We investigated the interactions between visual, tactile and auditory sensory signals for the perception of sequences of events. Sequences of flashes, taps and beeps were presented simultaneously. For each session, subjects were instructed to count the number of events presented in one modality (Target) and to ignore the stimuli presented in the other modalities (Background). The number of events presented in the background sequence could differ from the number of events in the target sequence. For each session, we quantified the Background-evoked bias by comparing subjects’ responses with and without Background (Target presented alone). Nine combinations between vision, touch and audition were tested. In each session but two, the Background significantly biased the Target. Vision was the most susceptible to Background-evoked bias and the least efficient in biasing the other two modalities. By contrast, audition was the least susceptible to Background-evoked bias and the most efficient in biasing the other two modalities. These differences were strongly correlated to the relative reliability of each modality. In line with this, the evoked biases were larger when the Background consisted of two instead of only one modality. These results show that for the perception of sequences of events: (1) vision, touch and audition are automatically integrated; (2) the respective contributions of the three modalities to the integrated percept differ; (3) the relative contribution of each modality depends on its relative reliability (1/variability); (4) task-irrelevant stimuli have more weight when presented in two rather than only one modality.</dcterms:abstract>
        <dc:date>2008-04-15</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0361923008000129</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 11:08:42</dcterms:dateSubmitted>
        <bib:pages>753-760</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0361-9230">
        <dcterms:isPartOf>
            <bib:Series>
               <dc:title>Special Issue: Robotics and Neuroscience</dc:title>
            </bib:Series>
        </dcterms:isPartOf>
        <prism:volume>75</prism:volume>
        <dc:title>Brain Research Bulletin</dc:title>
        <dc:identifier>DOI 10.1016/j.brainresbull.2008.01.009</dc:identifier>
        <prism:number>6</prism:number>
        <dcterms:alternative>Brain Research Bulletin</dcterms:alternative>
        <dc:identifier>ISSN 0361-9230</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_687">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/687/S0361923008000129.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/S0361923008000129?casa_token=oxu-chvsW1cAAAAA:QJieAmBd6sBaerXF9Ku4jk2NkRTfwvfqNyKL4sSFx6TK_vXp_LGk8gRVApfKJsiE2zeDYtKF9GE</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 11:08:51</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://doi.org/10.1080/17415977.2018.1490279">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1741-5977"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Awan</foaf:surname>
                        <foaf:givenName>Fahim Gohar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saleem</foaf:surname>
                        <foaf:givenName>Omer</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kiran</foaf:surname>
                        <foaf:givenName>Asima</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_689"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>47A52</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>65F22</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>92C55</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Electroencephalography</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>inverse problem</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>low-resolution electrical tomography</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>minimum norm estimates</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>source localization</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Recent trends and advances in solving the inverse problem for EEG source localization</dc:title>
        <dcterms:abstract>This paper addresses the recent advancements and trends in the field of electroencephalography (EEG) using inverse problem solutions. Using the EEG data of the brain to gather the information regarding the neuronal current source distribution has been a persisting challenge. Since the EEG inverse problem is ill-posed in nature; therefore, it does not offer a unique result. A trivial and precise solution yields a detailed insight regarding the electrical activity as well as the damaged tissue in the brain. Ordinarily, this problem is solved using the regularization techniques, such as minimum norm estimates, mixed-norm estimate, low-resolution electrical tomography, artificial neural networks, and their modified variants. In this paper, the latest algorithmic developments in solving the EEG inverse problem are reviewed. The optimization rendered by these techniques in accurately solving the neural source localization problem is also discussed. The comparative performance analysis of the recent techniques has been presented. Furthermore, a number of future enhancements have also been proposed to further improve the performance of these state-of-the-art techniques.</dcterms:abstract>
        <dc:date>2019-11-02</dc:date>
        <z:libraryCatalog>Taylor and Francis+NEJM</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://doi.org/10.1080/17415977.2018.1490279</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 11:46:37</dcterms:dateSubmitted>
        <dc:description>Publisher: Taylor &amp; Francis
_eprint: https://doi.org/10.1080/17415977.2018.1490279</dc:description>
        <bib:pages>1521-1536</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1741-5977">
        <prism:volume>27</prism:volume>
        <dc:title>Inverse Problems in Science and Engineering</dc:title>
        <dc:identifier>DOI 10.1080/17415977.2018.1490279</dc:identifier>
        <prism:number>11</prism:number>
        <dc:identifier>ISSN 1741-5977</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_689">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/689/Awan et al. - 2019 - Recent trends and advances in solving the inverse .pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.tandfonline.com/doi/pdf/10.1080/17415977.2018.1490279</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 11:46:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_691">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <prism:volume>1</prism:volume><prism:number>1</prism:number>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pascual-Marqui</foaf:surname>
                        <foaf:givenName>Roberto Domingo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_690"/>
        <dc:title>Review of Methods for Solving the EEG Inverse Problem</dc:title>
        <dcterms:abstract>This paper reviews the class of instantaneous, 3D, discrete, linear solutions for the EEG inverse problem. Five different inverse methods are analyzed and compared: minimum norm, weighted minimum norm, Backus and Gilbert, weighted resolution optimization (WROP), and low resolution brain electromagnetic tomography (LORETA). The inverse methods are compared by testing localization errors in the estimation of single and multiple sources. These tests constitute the minimum necessary condition to be satisfied by any tomography. Of the five inverse solutions tested, only LORETA demonstrates the ability of correct localization in 3D space. The other four inverse solutions should not be used if the research aim is to localize the neuronal generators of EEG in a 3D brain. In this sense, minimum norm, weighted minimum norm, Backus and Gilbert, and WROP can be likened to x-rays, where depth information is totally lacking. For the sake of reproducible research, all the material and methods used in this part of the study, consisting of computer programs (source code and executables) and data, are available upon request to the author. In this way, all the results and conclusions can be checked, reproduced, and validated by the interested reader.</dcterms:abstract>
        <dc:date>1999</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_690">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/690/Pascual-Marqui - 1999 - Review of Methods for Solving the EEG Inverse Prob.pdf"/>
        <dc:title>Pascual-Marqui - 1999 - Review of Methods for Solving the EEG Inverse Prob.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.brainmaster.com/software/pubs/brain/loreta/TechnicalDetails.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 11:46:42</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.frontiersin.org/articles/10.3389/fnint.2010.00009">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1662-5145"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Froyen</foaf:surname>
                        <foaf:givenName>Dries</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Van Atteveldt</foaf:surname>
                        <foaf:givenName>Nienke</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Blomert</foaf:surname>
                        <foaf:givenName>Leo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_693"/>
        <dc:title>Exploring the role of low level visual processing in letter-speech sound integration: a visual MMN study</dc:title>
        <dcterms:abstract>In contrast with for example audiovisual speech, the relation between visual and auditory properties of letters and speech sounds is artificial and learned only by explicit instruction. The arbitrariness of the audiovisual link together with the widespread usage of letter–speech sound pairs in alphabetic languages makes those audiovisual objects a unique subject for crossmodal research. Brain imaging evidence has indicated that heteromodal areas in superior temporal, as well as modality-specific auditory cortex are involved in letter–speech sound processing. The role of low level visual areas, however, remains unclear. In this study the visual counterpart of the auditory mismatch negativity (MMN) is used to investigate the influences of speech sounds on letter processing. Letter and non-letter deviants were infrequently presented in a train of standard letters, either in isolation or simultaneously with speech sounds. Although previous findings showed that letters systematically modulate speech sound processing (reflected by auditory MMN amplitude modulation), the reverse does not seem to hold: our results did not show evidence for an automatic influence of speech sounds on letter processing (no visual MMN amplitude modulation). This apparent asymmetric recruitment of low level sensory cortices during letter–speech sound processing, contrasts with the symmetric involvement of these cortices in audiovisual speech processing, and is possibly due to the arbitrary nature of the link between letters and speech sounds.</dcterms:abstract>
        <dc:date>2010</dc:date>
        <z:shortTitle>Exploring the role of low level visual processing in letter-speech sound integration</z:shortTitle>
        <z:libraryCatalog>Frontiers</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fnint.2010.00009</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 12:17:16</dcterms:dateSubmitted>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1662-5145">
        <prism:volume>4</prism:volume>
        <dc:title>Frontiers in Integrative Neuroscience</dc:title>
        <dc:identifier>ISSN 1662-5145</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_693">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/693/Froyen et al. - 2010 - Exploring the role of low level visual processing .pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fnint.2010.00009/pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 12:17:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.sciencedirect.com/science/article/pii/001346949390120K">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0013-4694"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Imada</foaf:surname>
                        <foaf:givenName>T.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hari</foaf:surname>
                        <foaf:givenName>R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Loveless</foaf:surname>
                        <foaf:givenName>N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>McEvoy</foaf:surname>
                        <foaf:givenName>L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sams</foaf:surname>
                        <foaf:givenName>M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_695"/>
        <link:link rdf:resource="#item_696"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>(Human)</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Auditory cortex</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Current dipole</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Magnetoencephalography</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Memory trace</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Mismatch field</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Oddball paradigm</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Determinants of the auditory mismatch response</dc:title>
        <dcterms:abstract>The auditory mismatch field (MMF) is supposed to reflect a comparison process between an infrequent deviant stimulus and the memory trace left by frequent standard stimuli. Therefore, the MMF amplitude has been thought to depend on the strength of such a trace. We examined this hypothesis in records with a 24-channel planar SQUID magnetometer by varying the number of stimuli preceding each deviant, the interdeviant interval (IDI) and the interstimulus interval (ISI) just preceding the deviant (pISI). When a constant IDI was employed and the number of standards between two deviants varied in different sessions, MMF amplitude increased as the number of standards increased. However, MMF did not depend on the number of standards between two deviants when the number varied within a single session and ISI varied as well. MMF decreased slightly when pISI increased from 0.6 to 3.4 sec. When IDI increased and the ISI remained constant, MMF amplitude increased. Most results can be explained within the framework of the memory-trace hypothesis of MMF generation. However, the strengthening of the trace seems to be a complex process which is also affected by the temporal features of the stimulus sequence.</dcterms:abstract>
        <dc:date>1993-09-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>ScienceDirect</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/001346949390120K</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 15:49:50</dcterms:dateSubmitted>
        <bib:pages>144-153</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0013-4694">
        <prism:volume>87</prism:volume>
        <dc:title>Electroencephalography and Clinical Neurophysiology</dc:title>
        <dc:identifier>DOI 10.1016/0013-4694(93)90120-K</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>Electroencephalography and Clinical Neurophysiology</dcterms:alternative>
        <dc:identifier>ISSN 0013-4694</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_695">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/695/Imada et al. - 1993 - Determinants of the auditory mismatch response.pdf"/>
        <dc:title>ScienceDirect Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/001346949390120K/pdf?md5=fd69a430531bb254743fb3045fbd99c2&amp;pid=1-s2.0-001346949390120K-main.pdf&amp;isDTMRedir=Y</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 15:49:52</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_696">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/696/001346949390120K.html"/>
        <dc:title>ScienceDirect Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.sciencedirect.com/science/article/pii/001346949390120K</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 15:49:55</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Report rdf:about="http://biorxiv.org/lookup/doi/10.1101/2022.02.11.480053">
        <z:itemType>report</z:itemType>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Neuroscience</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ghosh</foaf:surname>
                        <foaf:givenName>Priyanka</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Talwar</foaf:surname>
                        <foaf:givenName>Siddharth</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Banerjee</foaf:surname>
                        <foaf:givenName>Arpan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_700"/>
        <dc:title>Spatiotemporal mapping of the neural markers of prediction error processing across multisensory and unisensory modalities</dc:title>
        <dcterms:abstract>Prediction errors in the brain are indexed by two event-related potentials – MMN and P300, which are elicited upon violation of regularity in the occurrence of repetitive stimuli. While MMN reflects the brain's ability to perform automatic comparisons between consecutive stimuli and provides an electrophysiological index of sensory error detection, P300 is associated with cognitive processes such as update in working memory. Till date, there has been extensive research on the roles of MMN and P300 individually, because of their potential to be used as clinical markers of consciousness and attention, respectively. However, the relationship between these two ERPs, specifically in terms of their underlying cortical generators, in context of prediction error propagation along the hierarchical brain across multiple modalities is an open question. Our objective in this article is two-fold. First, we reconfirm previous reports regarding the generators of MMN and P300 in sensor space through source-space analysis using an accurate individual subject level co-registration of MRI and EEG data collected from healthy humans. We demonstrate that in multisensory environments, MMN and P300 markers represent “modality-specific” and “modality-independent” information processing, respectively. Advancing an earlier understanding that multisensory contexts speed up early sensory processing, our study reveals that this temporal facilitation extends to even the later components of prediction error processing, using custom-designed experiments that allow comparisons across different modality combinations. Such knowledge can be of immense value in clinical research for determining the stages of various treatments in aging, schizophrenia and depression, and their efficacy on cognitive function.</dcterms:abstract>
        <dc:date>2022-02-12</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://biorxiv.org/lookup/doi/10.1101/2022.02.11.480053</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 19:49:44</dcterms:dateSubmitted>
        <dc:description>DOI: 10.1101/2022.02.11.480053</dc:description>
        <z:type>preprint</z:type>
    </bib:Report>
    <z:Attachment rdf:about="#item_700">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/700/Ghosh et al. - 2022 - Spatiotemporal mapping of the neural markers of pr.pdf"/>
        <dc:title>Ghosh et al. - 2022 - Spatiotemporal mapping of the neural markers of pr.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.biorxiv.org/content/biorxiv/early/2022/10/02/2022.02.11.480053.full.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 19:49:40</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_702">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>15</prism:volume>
                <dc:title>Pakistan journal of biological sciences: PJBS</dc:title>
                <dc:identifier>DOI 10.3923/pjbs.2012.542.546</dc:identifier>
                <dcterms:alternative>Pakistan journal of biological sciences: PJBS</dcterms:alternative>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sittiprapaporn</foaf:surname>
                        <foaf:givenName>Phakkharawat</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_704"/>
        <link:link rdf:resource="#item_703"/>
        <dc:title>An Effect of Attention on Mismatch Negativity in Audiovisual Visual Modalities</dc:title>
        <dcterms:abstract>It remains unclear whether there is an analogous automatic deviant-related negativity elicited outside the auditory modality even though the MMN can be elicited in auditory modality. The present study employed simultaneous audio-visual stimulus in the oddball paradigm to re-examine the effects of attention on visual mismatch negativity in audiovisual perception. The electrical brain activities were recorded from normal, from normal participants subjects. Stimuli consisted of a set of four audio-visual stimuli that are distinguished by frequencies (Hz) for audio and features for visual appearing on the computer screen. ANOVA showed statistically significant of the interaction between electrode site and modality. The difference waves with 100-200 msec latency at the anterior sites were markedly different to the posterior sites. The emergence of posterior negativity in the audio-visual modality might not be attributed to visual discrimination process as it did not appear in the visual modality.</dcterms:abstract>
        <dc:date>2012-06-01</dc:date>
        <z:libraryCatalog>ResearchGate</z:libraryCatalog>
        <bib:pages>542-6</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_704">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/704/Sittiprapaporn - 2012 - An Effect of Attention on Mismatch Negativity in A.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Phakkharawat-Sittiprapaporn/publication/258313266_An_Effect_of_Attention_on_Mismatch_Negativity_in_Audiovisual_Visual_Modalities/links/5960fe1c458515a3570633be/An-Effect-of-Attention-on-Mismatch-Negativity-in-Audiovisual-Visual-Modalities.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 19:56:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_703">
        <z:itemType>attachment</z:itemType>
        <dc:title>ResearchGate Link</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.researchgate.net/profile/Phakkharawat-Sittiprapaporn/publication/258313266_An_Effect_of_Attention_on_Mismatch_Negativity_in_Audiovisual_Visual_Modalities/links/5960fe1c458515a3570633be/An-Effect-of-Attention-on-Mismatch-Negativity-in-Audiovisual-Visual-Modalities.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 19:56:44</dcterms:dateSubmitted>
        <z:linkMode>3</z:linkMode>
    </z:Attachment>
    <bib:Article rdf:about="https://www.frontiersin.org/articles/10.3389/fnhum.2021.721476">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>15</prism:volume>
                <dc:title>Frontiers in Human Neuroscience</dc:title>
                <dc:identifier>ISSN 1662-5161</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shiramatsu</foaf:surname>
                        <foaf:givenName>Tomoyo Isoguchi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mori</foaf:surname>
                        <foaf:givenName>Kanato</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ishizu</foaf:surname>
                        <foaf:givenName>Kotaro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Takahashi</foaf:surname>
                        <foaf:givenName>Hirokazu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_706"/>
        <dc:title>Auditory, Visual, and Cross-Modal Mismatch Negativities in the Rat Auditory and Visual Cortices</dc:title>
        <dcterms:abstract>When the brain tries to acquire an elaborate model of the world, multisensory integration should contribute to building predictions based on the various pieces of information, and deviance detection should repeatedly update these predictions by detecting “errors” from the actual sensory inputs. Accumulating evidence such as a hierarchical organization of the deviance-detection system indicates that the deviance-detection system can be interpreted in the predictive coding framework. Herein, we targeted mismatch negativity (MMN) as a type of prediction-error signal and investigated the relationship between multisensory integration and MMN. In particular, we studied whether and how cross-modal information processing affected MMN in rodents. We designed a new surface microelectrode array and simultaneously recorded visual and auditory evoked potentials from the visual and auditory cortices of rats under anesthesia. Then, we mapped MMNs for five types of deviant stimuli: single-modal deviants in (i) the visual oddball and (ii) auditory oddball paradigms, eliciting single-modal MMN; (iii) congruent audio-visual deviants, (iv) incongruent visual deviants, and (v) incongruent auditory deviants in the audio-visual oddball paradigm, eliciting cross-modal MMN. First, we demonstrated that visual MMN exhibited deviance detection properties and that the first-generation focus of visual MMN was localized in the visual cortex, as previously reported in human studies. Second, a comparison of MMN amplitudes revealed a non-linear relationship between single-modal and cross-modal MMNs. Moreover, congruent audio-visual MMN exhibited characteristics of both visual and auditory MMNs—its latency was similar to that of auditory MMN, whereas local blockage of N-methyl-D-aspartic acid receptors in the visual cortex diminished it as well as visual MMN. These results indicate that cross-modal information processing affects MMN without involving strong top-down effects, such as those of prior knowledge and attention. The present study is the first electrophysiological evidence of cross-modal MMN in animal models, and future studies on the neural mechanisms combining multisensory integration and deviance detection are expected to provide electrophysiological evidence to confirm the links between MMN and predictive coding theory.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:libraryCatalog>Frontiers</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fnhum.2021.721476</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 20:06:44</dcterms:dateSubmitted>
    </bib:Article>
    <z:Attachment rdf:about="#item_706">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/706/Shiramatsu et al. - 2021 - Auditory, Visual, and Cross-Modal Mismatch Negativ.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.frontiersin.org/articles/10.3389/fnhum.2021.721476/pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 20:06:47</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2007.00599.x">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>45</prism:volume>
                <dc:title>Psychophysiology</dc:title>
                <dc:identifier>DOI 10.1111/j.1469-8986.2007.00599.x</dc:identifier>
                <prism:number>1</prism:number>
                <dc:identifier>ISSN 1469-8986</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Horváth</foaf:surname>
                        <foaf:givenName>János</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Czigler</foaf:surname>
                        <foaf:givenName>István</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jacobsen</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Maess</foaf:surname>
                        <foaf:givenName>Burkhard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schröger</foaf:surname>
                        <foaf:givenName>Erich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winkler</foaf:surname>
                        <foaf:givenName>István</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_708"/>
        <link:link rdf:resource="#item_709"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Attention switching</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Auditory change detection</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>ERP</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Frequency change</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Magnitude of deviance</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Memory updating</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Mismatch negativity (MMN)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>N1</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Stimulus representation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>MMN or no MMN: No magnitude of deviance effect on the MMN amplitude</dc:title>
        <dcterms:abstract>Based on results showing that the “deviant-minus-standard” estimate of the mismatch negativity (MMN) amplitude increases with increasing amounts of deviance, it has been suggested that the MMN amplitude reflects the amount of difference between the neural representations of the standard and the deviant sound. However, the deviant-minus-standard waveform also includes an N1 difference. We tested the effects of the magnitude of deviance on MMN while minimizing this N1 confound. We found no significant magnitude of deviance effect on the genuine MMN amplitude. Thus we suggest that the average MMN amplitude does not reflect the difference between neural stimulus representations; rather it may index the percentage of detected deviants, each of which elicits an MMN response of uniform amplitude. These results are compatible with an explanation suggesting that MMN is involved in maintaining a neural representation of the auditory environment.</dcterms:abstract>
        <dc:date>2008</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>MMN or no MMN</z:shortTitle>
        <z:libraryCatalog>Wiley Online Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2007.00599.x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 20:18:53</dcterms:dateSubmitted>
        <dc:description>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2007.00599.x</dc:description>
        <bib:pages>60-69</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_708">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/708/Horváth et al. - 2008 - MMN or no MMN No magnitude of deviance effect on .pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1469-8986.2007.00599.x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 20:18:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_709">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/709/j.1469-8986.2007.00599.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2007.00599.x?casa_token=8rkO63Rbr_sAAAAA:v02YpaI1ES5CeAobdg-uHoahxFke0vZtftSxfBWyKcxZXqhWoM3fqm4kHl5vqqtt6jP672P3SGtzx3-u</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 20:18:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2005.00256.x">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>42</prism:volume>
                <dc:title>Psychophysiology</dc:title>
                <dc:identifier>DOI 10.1111/j.1469-8986.2005.00256.x</dc:identifier>
                <prism:number>1</prism:number>
                <dc:identifier>ISSN 1469-8986</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Näätänen</foaf:surname>
                        <foaf:givenName>Risto</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jacobsen</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Winkler</foaf:surname>
                        <foaf:givenName>István</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_711"/>
        <link:link rdf:resource="#item_712"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Auditory change</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Auditory event-related potential (AERP)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Feature-detector adaptation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Mismatch negativity (MMN)</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>N1</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Memory-based or afferent processes in mismatch negativity (MMN): A review of the evidence</dc:title>
        <dcterms:abstract>The mismatch negativity (MMN) is an electromagnetic response to any discriminable change in regular auditory input. This response is usually interpreted as being generated by an automatic cortical change-detection process in which a difference is found between the current input and the representation of the regular aspects of the preceding auditory input. Recently, this interpretation was questioned by Jääskeläinen et al. (2004) who proposed that the MMN is a product of an N1 (N1a) difference wave emerging in the subtraction procedure used to visualize and quantify the MMN. We now evaluate this “adaptation hypothesis” of the MMN in the light of the available data. It is shown that the MMN cannot be accounted for by differential activation of the afferent N1 transient detectors by repetitive (“standard”) stimuli and deviant (“novel”) stimuli and that the presence of a memory representation of the standard is required for the elicitation of MMN.</dcterms:abstract>
        <dc:date>2005</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Memory-based or afferent processes in mismatch negativity (MMN)</z:shortTitle>
        <z:libraryCatalog>Wiley Online Library</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2005.00256.x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 20:33:09</dcterms:dateSubmitted>
        <dc:description>_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2005.00256.x</dc:description>
        <bib:pages>25-32</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_711">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/711/Näätänen et al. - 2005 - Memory-based or afferent processes in mismatch neg.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1469-8986.2005.00256.x</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 20:33:10</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_712">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/712/j.1469-8986.2005.00256.html"/>
        <dc:title>Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2005.00256.x?casa_token=QECeCsc7fPUAAAAA:8QJX2lLvatfRAsJrSCaJQl36Aezo4swLqreeVgDJ41iI2Kb8L3yNu_GEU-K5terLa8CepXq8u6TuLAfN</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 20:33:14</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://journals.sagepub.com/doi/pdf/10.1177/1745691620966791?casa_token=PaCtq4KJQsQAAAAA:wGexP0vSatkX822zYthQPpgK3_PHS4hwNLTvEwSY8FSwAxRxX7DJKrUq30KgsGf1mkJhJSsnjhAyJw">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <prism:volume>Vol. 16(2) 466–471</prism:volume>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kihlstrom</foaf:surname>
                        <foaf:givenName>John F.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dc:title>Ecological Validity and “Ecological Validity”</dc:title>
        <dcterms:abstract>Egon Brunswik coined the term ecological validity to refer to the correlation between perceptual cues and the states
and traits of a stimulus. Martin Orne adapted the term to refer to the generalization of experimental findings to the
real world outside the laboratory. Both are legitimate uses of the term because the ecological validity of the cues in an
experiment determines the ecological validity of the experiment itself.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://journals.sagepub.com/doi/pdf/10.1177/1745691620966791?casa_token=PaCtq4KJQsQAAAAA:wGexP0vSatkX822zYthQPpgK3_PHS4hwNLTvEwSY8FSwAxRxX7DJKrUq30KgsGf1mkJhJSsnjhAyJw</rdf:value>
            </dcterms:URI>
        </dc:identifier>
    </bib:Article>
    <bib:Article rdf:about="https://doi.org/10.1007/s10548-013-0335-5">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>27</prism:volume>
                <dc:title>Brain Topography</dc:title>
                <dc:identifier>DOI 10.1007/s10548-013-0335-5</dc:identifier>
                <prism:number>4</prism:number>
                <dcterms:alternative>Brain Topogr</dcterms:alternative>
                <dc:identifier>ISSN 1573-6792</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Morlet</foaf:surname>
                        <foaf:givenName>Dominique</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fischer</foaf:surname>
                        <foaf:givenName>Catherine</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_715"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Auditory ERPs</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Coma</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>MCS</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>MMN</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Novelty P3</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>VS</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>MMN and Novelty P3 in Coma and Other Altered States of Consciousness: A Review</dc:title>
        <dcterms:abstract>In recent decades, there has been a growing interest in the assessment of patients in altered states of consciousness. There is a need for accurate and early prediction of awakening and recovery from coma. Neurophysiological assessment of coma was once restricted to brainstem auditory and primary cortex somatosensory evoked potentials elicited in the 30 ms range, which have both shown good predictive value for poor coma outcome only. In this paper, we review how passive auditory oddball paradigms including deviant and novel sounds have proved their efficiency in assessing brain function at a higher level, without requiring the patient’s active involvement, thus providing an enhanced tool for the prediction of coma outcome. The presence of an MMN in response to deviant stimuli highlights preserved automatic sensory memory processes. Recorded during coma, MMN has shown high specificity as a predictor of recovery of consciousness. The presence of a novelty P3 in response to the subject’s own first name presented as a novel (rare) stimulus has shown a good correlation with coma awakening. There is now a growing interest in the search for markers of consciousness, if there are any, in unresponsive patients (chronic vegetative or minimally conscious states). We discuss the different ERP patterns observed in these patients. The presence of novelty P3, including parietal components and possibly followed by a late parietal positivity, raises the possibility that some awareness processes are at work in these unresponsive patients.</dcterms:abstract>
        <dc:date>2014-07-01</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>MMN and Novelty P3 in Coma and Other Altered States of Consciousness</z:shortTitle>
        <z:libraryCatalog>Springer Link</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://doi.org/10.1007/s10548-013-0335-5</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 21:48:29</dcterms:dateSubmitted>
        <bib:pages>467-479</bib:pages>
    </bib:Article>
    <z:Attachment rdf:about="#item_715">
        <z:itemType>attachment</z:itemType>
        <rdf:resource rdf:resource="files/715/Morlet and Fischer - 2014 - MMN and Novelty P3 in Coma and Other Altered State.pdf"/>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://link.springer.com/content/pdf/10.1007%2Fs10548-013-0335-5.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-11 21:48:31</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
</rdf:RDF>
